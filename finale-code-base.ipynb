{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6166880,"sourceType":"datasetVersion","datasetId":908655},{"sourceId":580800,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":433483,"modelId":450350}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers peft datasets\n\n# 1. Imports & Environment checks\nimport os\nimport json\nimport re\nimport random\nimport time\nfrom pathlib import Path\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom itertools import product\nfrom pprint import pprint\nfrom datasets import Dataset as HFDataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport evaluate\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\ntry:\n    import pynvml\n    pynvml.nvmlInit()\n    _NVML_AVAILABLE = True\nexcept Exception:\n    _NVML_AVAILABLE = False\n\nprint(\"Torch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"NVML available:\", _NVML_AVAILABLE)\nif _NVML_AVAILABLE:\n    print(\"Number GPUs:\", pynvml.nvmlDeviceGetCount())\n\n# Reproducibility\nGLOBAL_SEED = 42\ndef set_seed(seed=GLOBAL_SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed(GLOBAL_SEED)\n\n# 2. Paths - edit these for your environment\nCUAD_JSON_PATH = \"/kaggle/input/atticus-open-contract-dataset-aok-beta/CUAD_v1/CUAD_v1.json\"\nOUTPUT_DIR = Path(\"./dataset_processed\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)","metadata":{"_uuid":"5f57f518-3e0e-4ca2-948a-af196fc20556","_cell_guid":"1181de9e-b1cc-43cf-bdbd-73b3e5c84249","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Utility: GPU stats\ndef get_gpu_stats(index=0):\n    if not _NVML_AVAILABLE:\n        return {\"error\": \"pynvml not available\"}\n    handle = pynvml.nvmlDeviceGetHandleByIndex(index)\n    mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n    return {\n        \"gpu_index\": index,\n        \"mem_used_GB\": mem.used / (1024**3),\n        \"mem_total_GB\": mem.total / (1024**3),\n        \"gpu_util_pct\": util.gpu,\n        \"gpu_sm_pct\": util.memory\n    }\n\n# Quick GPU print\nif _NVML_AVAILABLE:\n    for i in range(pynvml.nvmlDeviceGetCount()):\n        print(\"GPU\", i, get_gpu_stats(i))\n\n# 4. Load CUAD JSON and parse into snippet-level DataFrame\ndef parse_cuad_json(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        cuad = json.load(f)\n\n    records = []\n    docs = cuad.get(\"data\", cuad) if isinstance(cuad, dict) else cuad\n    for doc in docs:\n        doc_title = doc.get(\"title\") or doc.get(\"document_id\") or doc.get(\"name\") or \"\"\n        for para in doc.get(\"paragraphs\", []):\n            context = para.get(\"context\", \"\").strip()\n            for qa in para.get(\"qas\", []):\n                question = qa.get(\"question\", \"\").strip()\n                # Extract label/clause robustly (use anything in quotes after related to, else fallback)\n                m = re.search(r'related to\\s+\"([^\"]+)\"', question, flags=re.IGNORECASE)\n                if not m:\n                    m2 = re.search(r'\"([^\"]+)\"', question)\n                    clause = m2.group(1).strip() if m2 else question\n                else:\n                    clause = m.group(1).strip()\n                if qa.get(\"is_impossible\", False):\n                    continue\n                answers = qa.get(\"answers\", [])\n                for a in answers:\n                    text = (a.get(\"text\") or \"\").strip()\n                    if not text:\n                        continue\n                    # Normalize whitespace\n                    text = re.sub(r\"\\s+\", \" \", text)\n                    # Filter out extremely short snippets\n                    if len(text.split()) < 3 or len(text) < 12:\n                        continue\n                    records.append({\"document\": doc_title, \"text\": text, \"label\": clause})\n    df = pd.DataFrame(records).drop_duplicates(subset=[\"text\", \"label\"]).reset_index(drop=True)\n    return df\n\ndf = parse_cuad_json(CUAD_JSON_PATH)\nprint(\"Total parsed snippets:\", len(df))\ndf.head(6)\n\n\n# 5. Basic EDA - counts, length distribution, top labels\ndf['text_len_words'] = df['text'].apply(lambda t: len(t.split()))\nlabel_counts = df['label'].value_counts()\n\nprint(\"\\nNumber of unique labels:\", label_counts.shape[0])\nprint(\"Top 20 labels:\\n\", label_counts.head(20).to_string())\n\nlabel_counts.to_csv(OUTPUT_DIR / \"label_counts.csv\")\n\n# Plot counts (bar for top 30)\nplt.figure(figsize=(14,5))\nlabel_counts.head(30).plot(kind='bar')\nplt.title(\"Top 30 Clause Label Counts\")\nplt.xlabel(\"Clause label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n\n# Text length histogram\nplt.figure(figsize=(10,4))\nplt.hist(df['text_len_words'], bins=50)\nplt.title(\"Distribution of snippet lengths (words)\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Number of snippets\")\nplt.show()\n\n\n# 6. Label encoding and stratified splits\nle = LabelEncoder()\ndf['label_id'] = le.fit_transform(df['label'])\nlabels = list(le.classes_)\nnum_labels = len(labels)\nprint(\"Number of classes (label encoder):\", num_labels)\n\n# Save mapping\nmapping_df = pd.DataFrame({\"label\": labels, \"label_id\": list(range(num_labels))})\nmapping_df.to_csv(OUTPUT_DIR / \"label_mapping.csv\", index=False)\njoblib.dump(le, OUTPUT_DIR / \"label_encoder.joblib\")\n\n# Stratified split: train 85%, val 7.5%, test 7.5% (same as your earlier split)\ntrain_df, temp_df = train_test_split(df, test_size=0.15, stratify=df['label_id'], random_state=GLOBAL_SEED)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label_id'], random_state=GLOBAL_SEED)\n\nprint(\"Train / Val / Test sizes:\", len(train_df), len(val_df), len(test_df))\n\n# Save CSVs for downstream training scripts\ntrain_df.to_csv(OUTPUT_DIR / \"train.csv\", index=False)\nval_df.to_csv(OUTPUT_DIR / \"val.csv\", index=False)\ntest_df.to_csv(OUTPUT_DIR / \"test.csv\", index=False)\n\n# Also save a small sample to sanity-check loading later\ntrain_df.sample(5, random_state=GLOBAL_SEED).to_csv(OUTPUT_DIR / \"train_sample.csv\", index=False)","metadata":{"_uuid":"1676b7cd-6a7d-444b-a79f-828e831ee26a","_cell_guid":"3254b76a-8ecd-4a27-a195-c9174864ace6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Quick sanity functions to read processed dataset (used later)\ndef load_processed_split(split=\"train\"):\n    p = OUTPUT_DIR / f\"{split}.csv\"\n    return pd.read_csv(p)\n\n# Re-use label encoder\nle = joblib.load(OUTPUT_DIR / \"label_encoder.joblib\")\nlabels = list(le.classes_)\nnum_labels = len(labels)\n\n# Load processed CSVs\ntrain_df = pd.read_csv(OUTPUT_DIR / \"train.csv\")\nval_df = pd.read_csv(OUTPUT_DIR / \"val.csv\")\ntest_df = pd.read_csv(OUTPUT_DIR / \"test.csv\")\n\n# Simple helper to make HF datasets\ndef make_hf_dataset(df):\n    return HFDataset.from_pandas(df[[\"text\",\"label_id\"]].rename(columns={\"label_id\":\"labels\"}))\n\ntrain_hf = make_hf_dataset(train_df)\nval_hf = make_hf_dataset(val_df)\ntest_hf = make_hf_dataset(test_df)\n\n# Tokenizer + model name (Legal-BERT)\nMODEL_NAME = \"nlpaueb/legal-bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\ndef tokenize_fn(examples, max_length=256):\n    return tokenizer(examples[\"text\"], truncation=True, max_length=max_length)\n\ntrain_hf = train_hf.map(lambda x: tokenize_fn(x, max_length=256), batched=True)\nval_hf = val_hf.map(lambda x: tokenize_fn(x, max_length=256), batched=True)\ntest_hf = test_hf.map(lambda x: tokenize_fn(x, max_length=256), batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Metrics helpers\ndef compute_metrics_from_preds(y_true, y_pred):\n    res = {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"precision_macro\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"recall_macro\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"precision_weighted\": precision_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n        \"recall_weighted\": recall_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n    }\n    return res\n\ndef bootstrap_ci(y_true, y_pred, metric_fn=f1_score, n_boot=1000, alpha=0.05, average=\"macro\"):\n    scores = []\n    n = len(y_true)\n    for _ in range(n_boot):\n        idx = np.random.choice(n, n, replace=True)\n        scores.append(metric_fn(np.array(y_true)[idx], np.array(y_pred)[idx], average=average))\n    lower = np.percentile(scores, 100*alpha/2)\n    upper = np.percentile(scores, 100*(1-alpha/2))\n    return np.mean(scores), (lower, upper)\n\n# Trainer compute_metrics wrapper\ndef trainer_compute_metrics(eval_pred):\n    logits, labels_ids = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return compute_metrics_from_preds(labels_ids, preds)\n\n# Custom TrainerCallback for logging GPU stats after each epoch\nfrom transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n\nclass GPUStatsCallback(TrainerCallback):\n    def __init__(self, nvml_available=_NVML_AVAILABLE, interval_sec=2):\n        self.nvml_available = nvml_available\n    def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n        if not self.nvml_available:\n            return\n        gpu_info = [get_gpu_stats(i) for i in range(pynvml.nvmlDeviceGetCount())]\n        # append to file\n        logpath = Path(args.output_dir) / \"gpu_epoch_stats.jsonl\"\n        with open(logpath, \"a\") as f:\n            f.write(json.dumps({\"epoch\": state.epoch, \"step\": state.global_step, \"time\": time.time(), \"gpu_info\": gpu_info}) + \"\\n\")\n\n# Function to create model (optionally wrap in LoRA)\ndef build_model(model_name=MODEL_NAME, num_labels=num_labels, lora=False, lora_r=8, lora_alpha=16, lora_dropout=0.1):\n    base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n    if lora:\n        peft_config = LoraConfig(\n            r=lora_r,\n            lora_alpha=lora_alpha,\n            target_modules=[\"query\", \"value\", \"dense\", \"key\"], # common targets; adjust if errors\n            lora_dropout=lora_dropout,\n            bias=\"none\",\n            task_type=\"SEQ_CLS\"\n        )\n        model = get_peft_model(base_model, peft_config)\n    else:\n        model = base_model\n    return model\n\n# CPU latency helper (use a small set of texts)\ndef measure_cpu_latency(predict_fn, samples, n_runs=50):\n    # move model to cpu inside predict_fn if needed\n    times = []\n    for _ in range(n_runs):\n        t0 = time.time()\n        predict_fn(samples)\n        t1 = time.time()\n        times.append(t1-t0)\n    return np.mean(times), np.std(times)\n\n# Prediction helper using the loaded trainer/model for inference\ndef predict_with_model(trainer_or_model, texts, batch_size=16, device=None):\n    # Accept either Trainer or model (PeftModel or base)\n    if hasattr(trainer_or_model, \"predict\"):\n        ds = HFDataset.from_dict({\"text\": texts}).map(lambda x: tokenizer(x[\"text\"], truncation=True, max_length=256), batched=True)\n        preds_out = trainer_or_model.predict(ds)\n        preds = np.argmax(preds_out.predictions, axis=1)\n        return preds\n    else:\n        # treat as model\n        model = trainer_or_model\n        model.eval()\n        if device:\n            model.to(device)\n        inputs = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=256)\n        with torch.no_grad():\n            outputs = model(**{k:v.to(model.device) for k,v in inputs.items()})\n            logits = outputs.logits.cpu().numpy()\n            preds = np.argmax(logits, axis=1)\n        return preds\n\n\nsearch_lrs = [2e-5]\nsearch_epochs = [10,20]\nsearch_batch = [,1632]\nseeds = [42,123]  \n\nresults = []\nruns_dir = Path(\"./runs\")\nruns_dir.mkdir(exist_ok=True)\n\nfor model_mode in [\"full\"]:\n    print(f\"\\n=== Starting experiments for mode: {model_mode} ===\")\n    for lr, epochs, batch_size in product(search_lrs, search_epochs, search_batch):\n        config_name = f\"{model_mode}_lr{lr}_ep{epochs}_bs{batch_size}\"\n        print(\"\\nConfig:\", config_name)\n        run_records = []\n        for seed in seeds:\n            set_seed(seed)\n            run_out = runs_dir / f\"{config_name}_seed{seed}\"\n            run_out.mkdir(parents=True, exist_ok=True)\n\n            # Build model (LoRA wraps base)\n            use_lora = model_mode == \"lora\"\n            model = build_model(MODEL_NAME, num_labels, lora=use_lora, lora_r=8, lora_alpha=16, lora_dropout=0.1)\n\n            # TrainingArguments\n            training_args = TrainingArguments(\n                output_dir=str(run_out),\n                per_device_train_batch_size=batch_size,\n                per_device_eval_batch_size=batch_size,\n                learning_rate=lr,\n                num_train_epochs=epochs,\n                eval_strategy=\"epoch\",\n                save_strategy=\"epoch\",\n                load_best_model_at_end=True,\n                metric_for_best_model=\"f1_macro\",\n                greater_is_better=True,\n                logging_steps=50,\n                save_total_limit=3,\n                seed=seed,\n                fp16=torch.cuda.is_available(),\n                report_to=\"none\"\n            )\n\n            # Trainer\n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=train_hf,\n                eval_dataset=val_hf,\n                processing_class=tokenizer,\n                data_collator=data_collator,\n                compute_metrics=trainer_compute_metrics,\n                callbacks=[EarlyStoppingCallback(early_stopping_patience=2), GPUStatsCallback()]\n            )\n\n            # Train\n            t0 = time.time()\n            trainer.train()\n            train_time = time.time() - t0\n\n            # Evaluate on validation (best model loaded)\n            eval_out = trainer.evaluate(eval_dataset=val_hf)\n\n            test_preds_out = trainer.predict(test_hf)\n            test_preds = np.argmax(test_preds_out.predictions, axis=1)\n            test_labels = test_preds_out.label_ids\n\n            # per-class report\n            report = classification_report(test_labels, test_preds, target_names=labels, output_dict=True, zero_division=0)\n            report_df = pd.DataFrame(report).transpose()\n            report_df.to_csv(run_out / \"classification_report_test.csv\")\n\n            # bootstrap CI for macro F1\n            f1_mean, (f1_low, f1_high) = bootstrap_ci(test_labels, test_preds, f1_score, n_boot=500, average=\"macro\")\n\n            # GPU stats file (last logged if any)\n            gpu_stats_file = run_out / \"gpu_epoch_stats.jsonl\"\n            gpu_stats = []\n            if gpu_stats_file.exists():\n                with open(gpu_stats_file, \"r\") as f:\n                    for l in f:\n                        gpu_stats.append(json.loads(l))\n\n            # Save artifacts: model & tokenizer\n            if use_lora:\n                # Save PEFT adapters (this will save only adapter weights)\n                trainer.model.save_pretrained(run_out / \"peft_adapter\")\n                tokenizer.save_pretrained(run_out / \"tokenizer\")\n            else:\n                trainer.save_model(run_out / \"full_model\")\n                tokenizer.save_pretrained(run_out / \"tokenizer\")\n\n            # CPU latency measurement (move model to cpu temporarily)\n            try:\n                # load best model for CPU test\n                if use_lora:\n                    base_m = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n                    peft_loaded = PeftModel.from_pretrained(base_m, run_out / \"peft_adapter\")\n                    peft_loaded = peft_loaded.merge_and_unload()  \n                    cpu_model = peft_loaded.to(\"cpu\")\n                else:\n                    cpu_model = AutoModelForSequenceClassification.from_pretrained(run_out / \"full_model\").to(\"cpu\")\n                sample_texts = val_df[\"text\"].sample(32, random_state=seed).tolist()\n                cpu_mean, cpu_std = measure_cpu_latency(lambda texts: predict_with_model(cpu_model, texts, device=\"cpu\"), sample_texts, n_runs=10)\n            except Exception as e:\n                cpu_mean, cpu_std = None, None\n                print(\"CPU latency measurement failed:\", e)\n\n            rec = {\n                \"config\": config_name,\n                \"seed\": seed,\n                \"model_mode\": model_mode,\n                \"lr\": lr,\n                \"epochs\": epochs,\n                \"batch_size\": batch_size,\n                \"train_time_s\": train_time,\n                \"val_metrics\": eval_out,\n                \"test_f1_macro\": f1_score(test_labels, test_preds, average=\"macro\"),\n                \"test_f1_weighted\": f1_score(test_labels, test_preds, average=\"weighted\"),\n                \"test_accuracy\": accuracy_score(test_labels, test_preds),\n                \"f1_macro_boot_mean\": f1_mean,\n                \"f1_macro_ci_low\": f1_low,\n                \"f1_macro_ci_high\": f1_high,\n                \"gpu_stats\": gpu_stats,\n                \"cpu_latency_mean_s\": cpu_mean,\n                \"cpu_latency_std_s\": cpu_std,\n                \"run_dir\": str(run_out)\n            }\n            run_records.append(rec)\n            # free memory\n            del trainer\n            del model\n            torch.cuda.empty_cache()\n        # aggregate seed runs for this config\n        results.extend(run_records)\n        # Save partial results after each config\n        pd.DataFrame(results).to_json(\"transformer_grid_results.jsonl\", orient=\"records\", lines=True)\n\n\n\n# Finalize results to CSV\nresults_df = pd.read_json(\"transformer_grid_results.jsonl\", lines=True)\nresults_df.to_csv(\"transformer_grid_results.csv\", index=False)\nprint(\"Saved transformer grid results to transformer_grid_results.csv\")\n\nres = pd.read_csv(\"transformer_grid_results.csv\")\n# explode val_metrics column if present\ndef extract_metric(d, key):\n    try:\n        return json.loads(d).get(key)\n    except Exception:\n        return None\n\n# example plot: test F1 by lr for LoRA vs Full (mean across seeds)\nagg = res.groupby([\"model_mode\",\"lr\"])[\"test_f1_macro\"].mean().reset_index()\nplt.figure(figsize=(8,5))\nfor mode in agg[\"model_mode\"].unique():\n    sub = agg[agg[\"model_mode\"]==mode]\n    plt.plot(sub[\"lr\"], sub[\"test_f1_macro\"], marker=\"o\", label=mode)\nplt.xscale(\"log\")\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Mean test F1 (macro)\")\nplt.title(\"LR sweep: mean test macro-F1 across modes\")\nplt.legend()\nplt.show()\n\n# Per-class F1 for best run (choose highest test_f1_macro)\nbest_idx = res[\"test_f1_macro\"].idxmax()\nbest_run = res.loc[best_idx]\nprint(\"Best run record:\\n\", best_run.to_dict())\n\nbest_report = pd.read_csv(Path(best_run[\"run_dir\"]) / \"classification_report_test.csv\", index_col=0)\nbest_report = best_report.sort_values(\"f1-score\", ascending=False)\nplt.figure(figsize=(12,5))\nplt.bar(best_report.index[:-3], best_report[\"f1-score\"][:-3])\nplt.xticks(rotation=90)\nplt.ylabel(\"F1-Score\")\nplt.title(\"Per-class F1 (best run)\")\nplt.show()\n\nprint(\"Training + grid search complete. Results saved. Use transformer_grid_results.csv for numeric tables and per-run folders for artifacts.\")\n\n\ndef predict_clause(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(model.device)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n        pred_id = logits.argmax(dim=-1).item()\n    return {\n        \"text\": text,\n        \"predicted_label\": labels[pred_id],   # use true label names\n        \"confidence\": float(probs[pred_id])\n    }\n\ntexts = [\n    \"This Agreement shall be governed by the laws of the State of New York.\",\n    \"The liability of the supplier shall not exceed the total fees paid under this agreement.\"\n]\n\nfor t in texts:\n    print(predict_clause(t))","metadata":{"_uuid":"8c1dd43a-9a37-4921-9d74-ffaac99914ef","_cell_guid":"b643e5f7-0f34-45cd-aed2-222cc11a215c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}